{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADALINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing neccessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "''' Arguments : Weights id 2D vector (1,number of features + 1) --- +1 indicate bias             \n",
    "                Input is 1D vector (number_of_features,) \n",
    "'''\n",
    "def predict(Weights,Input):\n",
    "    ans = np.dot(Input, Weights[0,1:]) + Weights[0,0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Signum Function for predicting accurate label\n",
    " Because Adaline will give numeric result and that result \n",
    " shud be compared with threshold to decide the correct label\n",
    " here threshold is taken as 0.5 as label is 0 and 1\n",
    " so predicted values will lie between 0 and 1 \n",
    "'''\n",
    "def signum(x):\n",
    "    threshold = 0.5\n",
    "    if x >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaline Training Function\n",
    "''' Arguments : Input is 2D vector (m,n) where m is number of training example and n is number of features\n",
    "                Label is 1D vector (m,)\n",
    "                lr is Learning rate which is initialised with 0.1\n",
    "                iter is number of iteration which is initialised to 10\n",
    "'''\n",
    "def Adaline_Train(Input,Label,lr = 0.01,iter = 100):\n",
    "    m,n = Input.shape\n",
    "    # concatenating 1 to features vector for learning bias\n",
    "    Input = np.hstack((np.ones((m, 1)),Input))\n",
    "    # Initializing Weights with random value of shape (1,n+1) ..+1 is for bias\n",
    "    W = np.random.rand(n+1).reshape((1,n+1))\n",
    "    # for each iteration \n",
    "    for i in range(iter):\n",
    "        # for each training example\n",
    "        for j in range(m):\n",
    "            # predicting output for a example\n",
    "            Pred = np.dot(Input[j],W.T)\n",
    "            # updating all the weights including bias\n",
    "            W = W + lr * (Label[j] - Pred) * Input[j]\n",
    "    return W  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For OR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing input and target for OR\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,1,1,1]\n",
    "# Converting into numpy array\n",
    "Input = np.array(X)\n",
    "Label = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights For OR as a dataset:- \n",
      "[[0.4057526  0.31695103 0.42238106]]\n"
     ]
    }
   ],
   "source": [
    "# Training Adaline with OR dataset\n",
    "\n",
    "W = Adaline_Train( Input , Label )\n",
    "print(\"Weights For OR as a dataset:- \")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************** OR ***********************\n",
      "x1 x2  Output\n",
      "0  0   0\n",
      "1  0   1\n",
      "0  1   1\n",
      "1  1   1\n"
     ]
    }
   ],
   "source": [
    "# Checking for OR inputs\n",
    "print(\"********************** OR ***********************\")\n",
    "print(\"x1 x2  Output\")\n",
    "inputs = np.array([0, 0])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"0  0   {}\".format(res))\n",
    "\n",
    "inputs = np.array([1, 0])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"1  0   {}\".format(res))\n",
    "\n",
    "inputs = np.array([0, 1])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"0  1   {}\".format(res))\n",
    "\n",
    "inputs = np.array([1, 1])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"1  1   {}\".format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For AND Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing input and target for AND\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,0,0,1]\n",
    "# Converting into numpy array\n",
    "Input = np.array(X)\n",
    "Label = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights For AND as a dataset:- \n",
      "[[-0.01852431  0.50680045  0.18739587]]\n"
     ]
    }
   ],
   "source": [
    "# Training Adaline with AND dataset\n",
    "\n",
    "W = Adaline_Train( Input , Label ,0.1,10)\n",
    "print(\"Weights For AND as a dataset:- \")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************** AND ***********************\n",
      "x1 x2  Output\n",
      "0  0   0\n",
      "1  0   0\n",
      "0  1   0\n",
      "1  1   1\n"
     ]
    }
   ],
   "source": [
    "# Checking for AND inputs\n",
    "print(\"********************** AND ***********************\")\n",
    "\n",
    "print(\"x1 x2  Output\")\n",
    "inputs = np.array([0, 0])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"0  0   {}\".format(res))\n",
    "\n",
    "inputs = np.array([1, 0])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"1  0   {}\".format(res))\n",
    "\n",
    "inputs = np.array([0, 1])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"0  1   {}\".format(res))\n",
    "\n",
    "inputs = np.array([1, 1])\n",
    "res = signum(predict(W,inputs))\n",
    "print(\"1  1   {}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Normalization is done to bring all the values in similar range.\n",
    " We need Data in similar range because training model will deviate \n",
    " so much to reach global minimum and it may be possible that it never\n",
    " reaches global minimum. So to handle this problem normalization is used.\n",
    " Here , I used Min-Max Normalization which will bring the dataset\n",
    " in the range [0,1] \n",
    "'''\n",
    "def Normalization(X):\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    X = (X - mn)/(mx - mn)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking dataset from sklearn library\n",
    "from sklearn import datasets\n",
    "data = datasets.load_iris() # Data has four features and 150 training examples and 3 classes each has 50 example\n",
    "X = data.data[0:100,0:2]    # Taking 100 example of two classes and two features\n",
    "Y = data.target[0:100]\n",
    "\n",
    "X = Normalization(X)       # Normalizing Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "m,n = X.shape\n",
    "l = [i for  i in range(0, m)]\n",
    "# Shuffling the Indexes\n",
    "np.random.shuffle(l)\n",
    "# Shuffling the Dataset according to Shuffled indexes\n",
    "X = X[l]\n",
    "Y = Y[l]\n",
    "partition = (int)( 0.7 * m)  # Partition index for dividing into training and testing dataset\n",
    "# Partitioning into training and testing dataset\n",
    "X_train = X[0:partition]\n",
    "Y_train = Y[0:partition]\n",
    "X_test  = X[partition:m]\n",
    "Y_test  = Y[partition:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights For Iris dataset:- \n",
      "[[ 0.66209123  1.32534033 -1.53226422]]\n"
     ]
    }
   ],
   "source": [
    "# Training Adaline to classify Setosa and versicolor\n",
    "W = Adaline_Train(X_train,Y_train,0.01,400)\n",
    "print(\"Weights For Iris dataset:- \")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy for test dataset\n",
    "y_pred = []\n",
    "m,n = X_test.shape\n",
    "for i in range(m):\n",
    "    y_pred.append(signum(predict(W,X_test[i])))\n",
    "Diff = y_pred - Y_test\n",
    "c = Diff[Diff == 0]\n",
    "print(\"Accuracy : {}\".format((float)(len(c)/m)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
