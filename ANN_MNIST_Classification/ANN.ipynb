{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Assignment and Explaination\n",
    "\n",
    "This Assignment is related to Neural Network with backpropagation for multi class classification. In this assignment, neural network model is trained with three different dataset i.e. Digits, Alphabets, and XOR .Details of which are discussed in their respective cells.\n",
    "\n",
    "### Neural Network Model:\n",
    "The general methodology to build a Neural Network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc).\n",
    "<blockquote>In this Assignment, model consist of three layers:\n",
    "<blockquote> 1. Input Layer: The size of this layer depends on the number of features of input.<br>\n",
    "             2. Hidden Layer: The size of this layer can be decided on our own.<br>\n",
    "             3. Output Layer: The size of this layer depends on number of classes<br>\n",
    "</blockquote></blockquote>\n",
    "\n",
    "2. Initialize the model's parameters:\n",
    "<blockquote>Since we are dealing with three layer, so we need weight W1,bias b1 (whose dimensions are discussed in respective cell)for layers between input and hidden layer, and weight W2,bias b2 (dimensions are discussed in respective cell) for layers between hidden layer and output layer.After deciding the dimensions, these parameters are initialised with some small random values.\n",
    "</blockquote>\n",
    "\n",
    "3. Loop for fixed number of iteration:\n",
    "<blockquote>\n",
    "    1. Implement Forward Propagation:\n",
    "    <blockquote>Forward Propagation is calculating the values for next layers nodes using previous input,weights and bias.\n",
    "        Z represent the value before applying activation function(sigmoid/softmax) and A repesents the value after applying activation function.So, Z = weight . previous_input + bias and A = function(Z) where function is sigmoid for hidden layer and softmax for outer layer.Reason for respective case is explained in their respective cells.\n",
    "    </blockquote>\n",
    "    2. Compute Cost:\n",
    "    <blockquote>Cost function selected in this assignment is Cross Entropy Loss whose formula is given where this function is implemented.\n",
    "    </blockquote>\n",
    "    3. Implement backward propagation to get the gradients:\n",
    "    <blockquote> Backward propagation gives the gradients with respect to all weights and bias.Gradients calculation with respect to  parameters is little complicated.So, to generalize we take derivative of Output with respect to Z and then take derivative of Z with respect to W. Derivative of softmax with cross-entropy loss is given in this link: <br>    https://deepnotes.io/softmax-crossentropy  \n",
    "    </blockquote>\n",
    "    4. Update Parameters:\n",
    "    <blockquote>After getting the gradients with respect to parameters, its time to update these parameters with help of its gradients.\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # For handling Arrays\n",
    "import pandas as pd                 # For handling DataFrames\n",
    "import matplotlib.pyplot as plt     # For visualizing images\n",
    "from sklearn import datasets        # Import datasets from Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Important functions to be used in this Assignment including NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: one_hot\n",
    "    \n",
    "    Arguments:\n",
    "        A -- Labels of shape (number of examples,1)\n",
    "        num_classes -- number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Converted -- vector which is one hot coded of \n",
    "                     dimension (number of examples,number of classes)\n",
    "        \n",
    "    Reason:\n",
    "        Since we are dealing with Multi Class neural network which gives Output of \n",
    "        dimension (number of examples,number of classes) with probabilty values of\n",
    "        each classes ,So to calculate cost we need same kind Actual Output Dimension\n",
    "        matrix ,hence we are converting into one hot coded mtrix.\n",
    "\"\"\"\n",
    "def one_hot(A, num_classes):\n",
    "    Identity_matrix = np.eye(num_classes)          # Converting into identity matrix\n",
    "    Converted = Identity_matrix[A.reshape(-1)]     # Converting into one hot coded vector\n",
    "    Converted = np.squeeze(Converted)              # Reducing the extra dimension if have\n",
    "    return Converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: Normalization\n",
    "    \n",
    "    Arguments:\n",
    "        X -- Input matrix of shape (number of examples,number of features)\n",
    "        \n",
    "    Returns:\n",
    "        X_Norm -- Normalized matrix of shape (number of examples,number of features)\n",
    "        \n",
    "    Reason:\n",
    "        Normalization is done to keep the all the values in the same range.\n",
    "        This normalization is not generalized one as it is Specifically designed for \n",
    "        this assignment as we are dealing with images which when converted into numbers/\n",
    "        pixels ,the values are in the range of 0 to 255 .So, to bring in range of 0 to 1,\n",
    "        it is divided by 255.\n",
    "\"\"\"\n",
    "def Normalization(X):\n",
    "    X_Norm = X/255      # Dividing by 255 (maximum value of pixels)\n",
    "    return X_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: layer_sizes\n",
    "    \n",
    "    Arguments:\n",
    "        X -- Input matrix of shape (number of examples,number of features)\n",
    "        Y -- Labels of shape (number of examples,number of classes)\n",
    "    \n",
    "    Returns:\n",
    "        n_x -- number of units in input layer\n",
    "        n_y -- number of units in output layer\n",
    "    Reason:\n",
    "        To make Our Assignment more Modular , this extra function is used \n",
    "        to send number of units in input layer and number of units in output \n",
    "        layer\n",
    "\"\"\"\n",
    "def layer_sizes(X, Y):\n",
    "    n_x =  X.shape[1]         # size of input layer\n",
    "    n_y =  Y.shape[1]         # size of output layer\n",
    "    return (n_x,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: initialize_parameters\n",
    "    \n",
    "    Arguments:\n",
    "        n_x -- number of units in input layer\n",
    "        n_h -- number of units in hidden layer\n",
    "        n_y -- number of units in output layer\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing our parameters:\n",
    "                      W1 -- weight matrix of shape (n_h, n_x)\n",
    "                      b1 -- bias vector of shape (n_h, 1)\n",
    "                      W2 -- weight matrix of shape (n_y, n_h)\n",
    "                      b2 -- bias vector of shape (n_y, 1)\n",
    "    Reason:\n",
    "        To initialize all the parameters with small random values \n",
    "        which are normally distributed (randn is used for that) \n",
    "\"\"\"\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) \n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: sigmoid\n",
    "\n",
    "    Arguments:\n",
    "        x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "        s -- sigmoid(x)\n",
    "    Reason:\n",
    "        To compute the sigmoid as it is one of the activation function to be\n",
    "        used in hidden layers which brings values in range 0 to 1\n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: softmax\n",
    "\n",
    "    Arguments:\n",
    "        A -- matrix of dimension (number of classes,number of examples)\n",
    "\n",
    "    Return:\n",
    "        s -- softmax(x)\n",
    "    Reason:\n",
    "        Softmax function gives probabilty of each classes.So, used in\n",
    "        Output layer of multi class neural network so that we can get\n",
    "        the probabilty of occurence of each class.\n",
    "        This function is specifically designed for this Neural network,\n",
    "        because it takes input of dimension (number of classes,number of example)\n",
    "        So, sum should be done as per axis = 0\n",
    "\"\"\"\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: forward_propagation\n",
    "    \n",
    "    Argument:\n",
    "        X -- input matrix of size ( m,n_x )\n",
    "        parameters -- python dictionary containing your parameters \n",
    "                       (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "        cache -- python dictionary containing:\n",
    "                 Z1 -- matrix of shape (n_h, m)\n",
    "                 A1 -- matrix of shape (n_h, m)\n",
    "                 Z2 -- matrix of shape (n_y, m)\n",
    "                 A2 -- matrix of shape (n_y, m)\n",
    "    Reason:\n",
    "        This Function is required to do forward proapagation to calcluate \n",
    "        the probability of the respective classes.\n",
    "\"\"\"\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = W1 @ X.T + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    # filling the dictionary\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: compute_cost\n",
    "    \n",
    "    Arguments:\n",
    "        A2 -- The softmax/categorical output of output layer, \n",
    "                of shape (number of classes, number of examples)\n",
    "        Y -- \"true\" labels one-hot matrix of shape \n",
    "                ( number of examples,number of classes )\n",
    "    \n",
    "    Returns:\n",
    "        cost -- cross-entropy cost / MSE for perceptron\n",
    "    \n",
    "    Reason:\n",
    "        In this assignment, I used Cross-entropy loss, or log loss, as a \n",
    "        cost function which measures the performance of a classification \n",
    "        model whose output is a probability value between 0 and 1.Since, \n",
    "        this model gives output as probabilties of classes, we used this.\n",
    "        Cross-entropy loss increases as the predicted probability diverges\n",
    "        from the actual label.It is represented as \n",
    "        C = -sum of p,a in P,A ( a * log(p))\n",
    "        where P is predicted and A is actual.\n",
    "    \n",
    "\"\"\"\n",
    "def compute_cost(A2, Y):\n",
    "    \n",
    "    m = Y.shape[0]                       # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2.T),Y)\n",
    "        \n",
    "    cost = - np.sum(logprobs)/m          # Summing up the error and dividing by m\n",
    "    \n",
    "    cost = float(np.squeeze(cost))       # making sure cost is the dimension we expect. \n",
    "                                         # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: backward_propagation\n",
    "    \n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing our parameters \n",
    "        cache      -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "        X          -- input matrix of shape (number of examples,number of features)\n",
    "        Y          -- \"true\" labels matrix of shape (number of examples,number of classes)\n",
    "    \n",
    "    Returns:\n",
    "        grads --  python dictionary containing gradients with respect to parameters\n",
    "                 dW1 -- matrix of shape (n_h, n_x)\n",
    "                 db1 -- matrix of shape (n_h, 1)\n",
    "                 dW2 -- matrix of shape (n_y, n_h)\n",
    "                 db2 -- matrix of shape (n_y, 1)\n",
    "    \n",
    "    Reason:\n",
    "        We need back propagation to update the parameters as per error we face in \n",
    "        different layers. Depending on the cost function took gradient of outer layer\n",
    "        changes but gradients of inner layer won't change as we use sigmoid function \n",
    "        for hidden layers.\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y.T                                # derivative with respect to Z2 i.e. o/p before activation in final o/p\n",
    "    dW2 = (dZ2 @ A1.T)/m                        # derivative with respect to W2 i.e. weights of o/p layer\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m    # derivative with respect to b2 i.e. bias of o/p layer\n",
    "    dZ1 = (W2.T @ dZ2) * (A1 * (1-A1))          # derivative with respect to Z1 i.e. o/p before activation after first hidden\n",
    "    dW1 = (dZ1 @ X)/m                           # derivative with respect to W1 i.e. weights of hidden layer\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims=True)/m    # derivative with respect to b1 i.e. bias of hidden layer\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: updates_parameters\n",
    "    \n",
    "    Arguments:\n",
    "        parameters    -- python dictionary containing your parameters \n",
    "        grads         -- python dictionary containing your gradients \n",
    "        learning_rate -- initialised to 0.1\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing your updated parameters\n",
    "    \n",
    "    Reason:\n",
    "        We need to update parameters as per the error we recieved in the output.\n",
    "\"\"\"\n",
    "def update_parameters(parameters, grads, learning_rate = 0.1):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    b2 = b2-learning_rate*db2\n",
    "\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: nn_model\n",
    "    \n",
    "    Arguments:\n",
    "        X              -- input dataset of shape (number of examples,number of features)\n",
    "        Y              -- labels of shape ( number of examples,number of classes)\n",
    "        n_h            -- size of the hidden layer\n",
    "        num_iterations -- Number of iterations in gradient descent loop\n",
    "        print_cost     -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- parameters learnt by the model.\n",
    "    \n",
    "    Reason:\n",
    "        This function is main Neural Network model which decides the number of units \n",
    "        in hidden layer , initializes weights and bias for network and run a loop of\n",
    "        gradient descent under which there is forward propagation ,cost computation,\n",
    "        backward propagation and updation of parameters.\n",
    "        This model has only one hidden layer ,and every function is made accordingly\n",
    "\"\"\"\n",
    "def nn_model(X, Y, n_h, num_iterations = 1000, print_cost=False):\n",
    "    n_x,n_y = layer_sizes(X, Y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations+1):\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(cache['A2'], Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "                \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: predict\n",
    "    \n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        X          -- input data of size (number of examples,number of features)\n",
    "    \n",
    "    Returns:\n",
    "        predictions -- A vector of shape (number of eaxmples,) which gives the \n",
    "                        prediction class \n",
    "    \n",
    "    Reason:\n",
    "        This function is required to predict classes for each examples in X\n",
    "    \n",
    "\"\"\"\n",
    "def predict(parameters, X):\n",
    "    \n",
    "    # Computes probabilities using forward propagation\n",
    "    cache = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Retrieving A2 i.e the output matrix of shape (number of classes,number of examples) from cache\n",
    "    A2 =cache['A2']\n",
    "    \n",
    "    # taking the class with maximum probability\n",
    "    predictions = np.argmax(A2,axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function Name: Accuracy\n",
    "    \n",
    "    Arguments:\n",
    "        predicted  -- Predicted classes vector of shape (number of examples,) \n",
    "        Y          -- labels of shape ( number of examples,number of classes)\n",
    "    \n",
    "    Returns:\n",
    "        accuracy -- float value which gives accuracy\n",
    "    \n",
    "    Reason:\n",
    "        This function is required to calculate percentage of correctly \n",
    "        classified classes by training the Neural Network model\n",
    "    \n",
    "\"\"\"\n",
    "def Accuracy(predicted,Y):\n",
    "    c = 0                       # variable to store the count of correctly classified classes\n",
    "    m = Y.shape[0]\n",
    "    for i in range(m):\n",
    "        if(predicted[i]==np.argmax(Y[i])):\n",
    "            c+=1\n",
    "    return float(c/m)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Z Alphabet Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link for the Alphabet dataset is:\n",
    "https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format?select=A_Z+Handwritten+Data.csv\n",
    "\n",
    "This A-Z Alphabet Dataset consist more than 3 Lakh examples in numerical/pixel form in csv format.\n",
    "This will take a lot of time to train and test. Since the accuracy doesn't matter for this assignment, so\n",
    "I decided to reduce the number of examples to 2600 with each class has 100 examples.\n",
    "\n",
    "So, code to reduce the data:\n",
    "***********************************************\n",
    "<code>\n",
    "data = pd.read_csv(\"A_Z_Handwritten_Data.csv\")\n",
    "data = np.array(data)\n",
    "\n",
    "Mdata=[]\n",
    "for i in range(26):\n",
    "    T=data[data[:,0]==i]\n",
    "    T=T[ np.random.choice( T.shape[0],100, False),:]\n",
    "    Mdata.extend(list(T))\n",
    "\n",
    "Mdata=np.array(Mdata)\n",
    "df=pd.DataFrame(Mdata)\n",
    "df.to_csv(\"Modified_Data.csv\",index=False)\n",
    "</code>\n",
    "\n",
    "************************************************\n",
    "New Modified Data is saved, Link to access the data is:\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Modified Alphabet Dataset\n",
    "\n",
    "This Dataset consist of 2600 example with 100 examples for each 26 classes. \n",
    "This Dataset is in csv format which contains numerical values/pixel values of the images.\n",
    "Dimension of each image 28x28 i.e. 784 features that will be input to the NN model.\n",
    "This Dataset consist of 2600 rows and 785 column and first column represent the classes\n",
    "where 0 repersent 'A',1 represent 'B' and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Alph_data:  (2600, 785)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset from csv file using Pandas DataFrame\n",
    "Alph_data = pd.read_csv(\"Modified_Data.csv\")\n",
    "\n",
    "print(\"Shape of Alph_data: \",Alph_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix of Shape:  (2600, 784)\n",
      "True Labels of Shape:  (2600, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting the DataFrame into numpy array\n",
    "Alph_data = np.array(Alph_data)\n",
    "\n",
    "# Separating into Input Matrix and True Labels\n",
    "X_Alph = Alph_data[:,1:785]\n",
    "Y_Alph = Alph_data[:,0:1]\n",
    "\n",
    "print(\"Input Matrix of Shape: \",X_Alph.shape)\n",
    "print(\"True Labels of Shape: \",Y_Alph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Label:  C\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Data with the help of matplotlib imshow\n",
    "\n",
    "# Generating random index between 0 to number of samples\n",
    "n_samp = X_Alph.shape[0]\n",
    "rand_ind = np.random.randint(n_samp)\n",
    "\n",
    "plt.imshow(X_Alph[rand_ind].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "\n",
    "# printing actual class after converting into char\n",
    "print(\"Correct Label: \",chr(Y_Alph[rand_ind]+65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels of Shape:  (2600, 26)\n"
     ]
    }
   ],
   "source": [
    "# Normalization done to bring all values in same range i.e. 0 to 1\n",
    "X_Alph = Normalization(X_Alph)\n",
    "\n",
    "# True Label vector is converted into one-hot coded vector (reason explained where function is declared..see above)\n",
    "Y_Alph = one_hot(Y_Alph,26)\n",
    "\n",
    "print(\"True Labels of Shape: \",Y_Alph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train :  (1819, 784)\n",
      "Y-train :  (1819, 26)\n",
      "X-test :  (781, 784)\n",
      "Y-test :  (781, 26)\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the dataset\n",
    "m,n = X_Alph.shape\n",
    "l = [i for  i in range(0, m)]\n",
    "# Shuffling the Indexes\n",
    "np.random.shuffle(l)\n",
    "# Shuffling the Dataset according to Shuffled indexes\n",
    "X_Alph = X_Alph[l]\n",
    "Y_Alph = Y_Alph[l]\n",
    "\n",
    "partition = (int)( 0.7 * m)  # Partition index for dividing into training and testing dataset\n",
    "# Partitioning into training and testing dataset\n",
    "X_Alph_train = X_Alph[0:partition]\n",
    "Y_Alph_train = Y_Alph[0:partition]\n",
    "X_Alph_test  = X_Alph[partition:m]\n",
    "Y_Alph_test  = Y_Alph[partition:m]\n",
    "\n",
    "print(\"X-train : \",X_Alph_train.shape)\n",
    "print(\"Y-train : \",Y_Alph_train.shape)\n",
    "print(\"X-test : \",X_Alph_test.shape)\n",
    "print(\"Y-test : \",Y_Alph_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit Dataset is loaded from Sklearn Dataset which is named as load_digits\n",
    "This Dataset consist of 1797 example for each 26 classes. \n",
    "This Dataset is directly loaded from sklearn so images are given numerical values/pixel values\n",
    "and target which gives the class label for that image.\n",
    "Dimension of each image 8x8 i.e. 64 features that will be input to the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix of Shape:  (1797, 8, 8)\n",
      "Target Matrix of Shape:  (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Separating Dataset into Input Matrix and its Label\n",
    "X_Dig = digits.images\n",
    "Y_Dig = digits.target\n",
    "\n",
    "print(\"Input Matrix of Shape: \",X_Dig.shape)\n",
    "print(\"Target Matrix of Shape: \",Y_Dig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Label:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Total Number of Samples\n",
    "n_samples = len(digits.images)\n",
    "rand_ind = np.random.randint(n_samples)\n",
    "\n",
    "plt.imshow(X_Dig[rand_ind],cmap=plt.cm.gray_r)\n",
    "print(\"Correct Label: \",Y_Dig[rand_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix of Shape:  (1797, 64)\n",
      "Target Matrix of Shape:  (1797, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping into (number of samples,64)\n",
    "X_Dig = X_Dig.reshape((n_samples, -1))\n",
    "Y_Dig = one_hot(Y_Dig,10)\n",
    "\n",
    "print(\"Input Matrix of Shape: \",X_Dig.shape)\n",
    "print(\"Target Matrix of Shape: \",Y_Dig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train :  (1257, 64)\n",
      "Y-train :  (1257, 10)\n",
      "X-test :  (540, 64)\n",
      "Y-test :  (540, 10)\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the dataset\n",
    "m,n = X_Dig.shape\n",
    "l = [i for  i in range(0, m)]\n",
    "# Shuffling the Indexes\n",
    "np.random.shuffle(l)\n",
    "# Shuffling the Dataset according to Shuffled indexes\n",
    "X_Dig = X_Dig[l]\n",
    "Y_Dig = Y_Dig[l]\n",
    "partition = (int)( 0.7 * m)  # Partition index for dividing into training and testing dataset\n",
    "# Partitioning into training and testing dataset\n",
    "X_Dig_train = X_Dig[0:partition]\n",
    "Y_Dig_train = Y_Dig[0:partition]\n",
    "X_Dig_test  = X_Dig[partition:m]\n",
    "Y_Dig_test  = Y_Dig[partition:m]\n",
    "\n",
    "print(\"X-train : \",X_Dig_train.shape)\n",
    "print(\"Y-train : \",Y_Dig_train.shape)\n",
    "print(\"X-test : \",X_Dig_test.shape)\n",
    "print(\"Y-test : \",Y_Dig_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN Model with 0-9 Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.265930\n",
      "Cost after iteration 1000: 0.421290\n",
      "Cost after iteration 2000: 0.239488\n",
      "Cost after iteration 3000: 0.167173\n",
      "Cost after iteration 4000: 0.130153\n",
      "Cost after iteration 5000: 0.103858\n"
     ]
    }
   ],
   "source": [
    "# Train a model with a n_h number of unit in hidden layer\n",
    "parameters_Dig = nn_model(X_Dig_train, Y_Dig_train, n_h = 32,\n",
    "                          num_iterations = 5000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Predictions of the model on Test Dataset of Digits\n",
    "predictions_Dig = predict(parameters_Dig, X_Dig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label:  1\n",
      "Predicted Label:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACpxJREFUeJzt3e9rnfUZx/HPZ1HZ/BlYuyFNXRRGQAZLJRSkoG3dRp2ie7AHLShUBn2kWDYQ3SP3D6h7MASpuoKdslUFEacTbHTC5kxrt1nTjq5kNKuuKSP4Y7DSeu1BTqHrMs6dnu/9I1ffLwjmJId8r0N5e9/n5OT+OiIEIKcvtD0AgPoQOJAYgQOJETiQGIEDiRE4kBiBA4kROJAYgQOJXVTHD12xYkWMjo7W8aNRk0OHDjW21unTpxtba2xsrLG1JGloaKiRdWZmZnTixAn3u18tgY+OjmpqaqqOH42arF+/vrG15ufnG1trz549ja0lScPDw42sMzExUel+nKIDiRE4kBiBA4kROJAYgQOJETiQGIEDiRE4kFilwG1vsn3I9mHbD9Y9FIAy+gZue0jSzyTdKul6SVtsX1/3YAAGV+UIvlbS4Yg4EhEnJT0n6c56xwJQQpXAV0k6etbt2d7XAHRclcAX+4uV/7mYuu1ttqdsT83NzQ0+GYCBVQl8VtLqs26PSDp27p0i4omImIiIiZUrV5aaD8AAqgT+rqSv277W9iWSNkt6qd6xAJTQ9+/BI+KU7XslvSZpSNJTEXGg9skADKzSBR8i4hVJr9Q8C4DCeCcbkBiBA4kROJAYgQOJETiQGIEDiRE4kBiBA4nVsrMJynj44YcbW+vNN99sbK2rrrqqsbWa3EVFam5nk6o4ggOJETiQGIEDiRE4kBiBA4kROJAYgQOJETiQGIEDiVXZ2eQp28dtv9/EQADKqXIE/7mkTTXPAaAGfQOPiLck/bOBWQAUxnNwILFigbN1EdA9xQJn6yKgezhFBxKr8muyZyX9TtKY7VnbP6h/LAAlVNmbbEsTgwAoj1N0IDECBxIjcCAxAgcSI3AgMQIHEiNwIDECBxJj66Il2Lp1a6Pr7dy5s9H1mjI+Pt7YWqOjo42t1UUcwYHECBxIjMCBxAgcSIzAgcQIHEiMwIHECBxIjMCBxAgcSKzKRRdX295je9r2Adv3NzEYgMFVeS/6KUk/ioh9tq+QtNf26xHxQc2zARhQlb3JPoyIfb3PP5E0LWlV3YMBGNySnoPbHpW0RtI7i3yPrYuAjqkcuO3LJT0vaXtEfHzu99m6COieSoHbvlgLce+KiBfqHQlAKVVeRbekJyVNR8Qj9Y8EoJQqR/B1ku6WtNH2/t7Hd2ueC0ABVfYme1uSG5gFQGG8kw1IjMCBxAgcSIzAgcQIHEiMwIHECBxIjMCBxJb93mSTk5ONrZV1r7Cmzc/Ptz3CBYMjOJAYgQOJETiQGIEDiRE4kBiBA4kROJAYgQOJETiQWJWLLn7R9h9s/7G3ddFPmhgMwOCqvFX135I2RsSnvcsnv2371xHx+5pnAzCgKhddDEmf9m5e3PuIOocCUEbVjQ+GbO+XdFzS6xHB1kXAMlAp8Ig4HRHjkkYkrbX9jUXuw9ZFQMcs6VX0iJiXNClpUy3TACiqyqvoK20P9z7/kqRvSTpY92AABlflVfSrJe20PaSF/yH8MiJerncsACVUeRX9T1rYExzAMsM72YDECBxIjMCBxAgcSIzAgcQIHEiMwIHECBxIbNlvXdSkRx99tNH1xsfHG1trw4YNja21ffv2xta60HEEBxIjcCAxAgcSI3AgMQIHEiNwIDECBxIjcCAxAgcSqxx479ro79nmemzAMrGUI/j9kqbrGgRAeVV3NhmRdJukHfWOA6CkqkfwxyQ9IOnzGmcBUFiVjQ9ul3Q8Ivb2uR97kwEdU+UIvk7SHbZnJD0naaPtZ869E3uTAd3TN/CIeCgiRiJiVNJmSW9ExF21TwZgYPweHEhsSVd0iYhJLewuCmAZ4AgOJEbgQGIEDiRG4EBiBA4kRuBAYgQOJEbgQGLLfuui9evXp1xLkubn5xtdrykzMzNtj3DB4AgOJEbgQGIEDiRG4EBiBA4kRuBAYgQOJEbgQGIEDiRW6Z1svSuqfiLptKRTETFR51AAyljKW1U3RMSJ2iYBUByn6EBiVQMPSb+xvdf2tjoHAlBO1VP0dRFxzPZXJL1u+2BEvHX2HXrhb5Oka665pvCYAM5HpSN4RBzr/fe4pBclrV3kPmxdBHRMlc0HL7N9xZnPJX1H0vt1DwZgcFVO0b8q6UXbZ+7/i4h4tdapABTRN/CIOCLpmw3MAqAwfk0GJEbgQGIEDiRG4EBiBA4kRuBAYgQOJEbgQGIEDiRG4EBiBA4kRuBAYgQOJEbgQGIEDiRG4EBiBA4kRuBAYpUCtz1se7ftg7anbd9Y92AABlf1uug/lfRqRHzf9iWSLq1xJgCF9A3c9pWSbpK0VZIi4qSkk/WOBaCEKqfo10mak/S07fds7+hdHx1Ax1UJ/CJJN0h6PCLWSPpM0oPn3sn2NttTtqfm5uYKjwngfFQJfFbSbES807u9WwvB/xe2LgK6p2/gEfGRpKO2x3pfukXSB7VOBaCIqq+i3ydpV+8V9COS7qlvJAClVAo8IvZLmqh5FgCF8U42IDECBxIjcCAxAgcSI3AgMQIHEiNwIDECBxIjcCCxqm9VRQuGh4cbW+vmm29ubK3JycnG1rrQcQQHEiNwIDECBxIjcCAxAgcSI3AgMQIHEiNwIDECBxLrG7jtMdv7z/r42Pb2JoYDMJi+b1WNiEOSxiXJ9pCkv0t6sea5ABSw1FP0WyT9NSL+VscwAMpaauCbJT272DfYugjonsqB9zY9uEPSrxb7PlsXAd2zlCP4rZL2RcQ/6hoGQFlLCXyL/s/pOYBuqhS47UslfVvSC/WOA6CkqnuT/UvSl2ueBUBhvJMNSIzAgcQIHEiMwIHECBxIjMCBxAgcSIzAgcQcEeV/qD0naal/UrpC0oniw3RD1sfG42rP1yKi71911RL4+bA9FRETbc9Rh6yPjcfVfZyiA4kROJBYlwJ/ou0BapT1sfG4Oq4zz8EBlNelIziAwjoRuO1Ntg/ZPmz7wbbnKcH2att7bE/bPmD7/rZnKsn2kO33bL/c9iwl2R62vdv2wd6/3Y1tzzSI1k/Re9da/4sWrhgzK+ldSVsi4oNWBxuQ7aslXR0R+2xfIWmvpO8t98d1hu0fSpqQdGVE3N72PKXY3inptxGxo3eh0UsjYr7tuc5XF47gayUdjogjEXFS0nOS7mx5poFFxIcRsa/3+SeSpiWtaneqMmyPSLpN0o62ZynJ9pWSbpL0pCRFxMnlHLfUjcBXSTp61u1ZJQnhDNujktZIeqfdSYp5TNIDkj5ve5DCrpM0J+np3tOPHbYva3uoQXQhcC/ytTQv7du+XNLzkrZHxMdtzzMo27dLOh4Re9uepQYXSbpB0uMRsUbSZ5KW9WtCXQh8VtLqs26PSDrW0ixF2b5YC3HviogsV6RdJ+kO2zNaeDq10fYz7Y5UzKyk2Yg4c6a1WwvBL1tdCPxdSV+3fW3vRY3Nkl5qeaaB2bYWnstNR8Qjbc9TSkQ8FBEjETGqhX+rNyLirpbHKiIiPpJ01PZY70u3SFrWL4pWumxynSLilO17Jb0maUjSUxFxoOWxSlgn6W5Jf7a9v/e1H0fEKy3OhP7uk7Srd7A5IumelucZSOu/JgNQny6cogOoCYEDiRE4kBiBA4kROJAYgQOJETiQGIEDif0HrsqbENfmvXIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking whether Classified Correctly or not for any random index\n",
    "\n",
    "rand_ind = np.random.randint(X_Dig_test.shape[0])\n",
    "plt.imshow(X_Dig_test[rand_ind].reshape(8,8),cmap=plt.cm.gray_r)\n",
    "print(\"True Label: \",np.argmax(Y_Dig_test[rand_ind]))\n",
    "\n",
    "print(\"Predicted Label: \",predictions_Dig[rand_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for hidden layer of size 32 (TEST_DATA):  89.62962962962962\n",
      "Accuracy for hidden layer of size 32 (TRAIN_DATA):  98.09069212410502\n"
     ]
    }
   ],
   "source": [
    "# Printing accuracy for Train Data and Test Data\n",
    "print(\"Accuracy for hidden layer of size 32 (TEST_DATA): \"\n",
    "      ,Accuracy(predictions_Dig,Y_Dig_test))\n",
    "predictions1_Dig = predict(parameters_Dig, X_Dig_train)\n",
    "print(\"Accuracy for hidden layer of size 32 (TRAIN_DATA): \"\n",
    "      ,Accuracy(predictions1_Dig,Y_Dig_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden units: 17.59259259259259 %\n",
      "Accuracy for 4 hidden units: 46.111111111111114 %\n",
      "Accuracy for 10 hidden units: 82.77777777777777 %\n",
      "Accuracy for 20 hidden units: 87.4074074074074 %\n",
      "Accuracy for 30 hidden units: 88.33333333333333 %\n",
      "Accuracy for 40 hidden units: 91.48148148148148 %\n",
      "Accuracy for 50 hidden units: 90.92592592592592 %\n"
     ]
    }
   ],
   "source": [
    "# Trying on different size of hidden layer unit\n",
    "hidden_layer_sizes = [1, 4, 10, 20, 30, 40, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    parameters = nn_model(X_Dig_train, Y_Dig_train, n_h, num_iterations = 5000)\n",
    "    predictions = predict(parameters, X_Dig_test)\n",
    "    accuracy = Accuracy(predictions,Y_Dig_test)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN Model with A-Z Alphabets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 19.774114\n",
      "Cost after iteration 1000: 0.533306\n",
      "Cost after iteration 2000: 0.125121\n",
      "Cost after iteration 3000: 0.054537\n",
      "Cost after iteration 4000: 0.034469\n",
      "Cost after iteration 5000: 0.025423\n"
     ]
    }
   ],
   "source": [
    "# Train a model with a n_h number of unit in hidden layer\n",
    "parameters_Alph = nn_model(X_Alph_train, Y_Alph_train, n_h = 256,\n",
    "                           num_iterations = 5000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Predictions of the model on Test Dataset of Alphabets\n",
    "predictions_Alph = predict(parameters_Alph, X_Alph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label:  D\n",
      "Predicted Label:  D\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADdJJREFUeJzt3WGIXfWZx/Hfs7F9kxYxZKLBxp1slWVFMDWXsJAyaCTFLJUkL2oSpGSxbIrUYKHIBolkJBak2HZDWIvTdWgKqZNia8wL2a1MhLFkKd4ZMjVtutugs81sYmaiYq0vpow+fTEnZYxz//fmnnPPuZPn+4Fw7z3PPec8XPKbc+/9n3v+5u4CEM/fVN0AgGoQfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQV1T5s6WL1/uvb29Ze4SCGViYkIXL160Vp6bK/xmdo+kA5KWSPoPd38y9fze3l7V6/U8uwSQUKvVWn5u22/7zWyJpH+XtEnSrZJ2mNmt7W4PQLnyfOZfJ+mMu7/h7n+WNCRpczFtAei0POG/UdLZeY8ns2UfY2a7zKxuZvXp6ekcuwNQpDzhX+hLhU/8PtjdB9y95u61np6eHLsDUKQ84Z+UtGre489JOpevHQBlyRP+1yTdYmarzezTkrZLOlZMWwA6re2hPnefNbOHJP2X5ob6Bt39N4V1BqCjco3zu/tLkl4qqBcAJeL0XiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCKnWKbiw+J06cSNbXr1+frK9YsaJhbe/evcl1d+/enawjH478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUrnF+M5uQ9L6kDyXNunutiKZQnjfffDNZv//++3Ntf2pqqmFtZGQkue7bb7+drN97773J+tq1a5P16Io4yecud79YwHYAlIi3/UBQecPvkn5hZqNmtquIhgCUI+/b/vXufs7MVkh62cx+5+4f+yCX/VHYJUk33XRTzt0BKEquI7+7n8tupyS9IGndAs8ZcPeau9d6enry7A5AgdoOv5ktNbPPXrov6UuSThXVGIDOyvO2/3pJL5jZpe38xN3/s5CuAHRc2+F39zck3V5gL6jA4OBgsj4xMdGxfT///PO56k899VSynrpewJ49e5LrRsBQHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt0d3PDwcNUttO2DDz5I1vv7+xvWZmdnk+s2u6z41YAjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTj/Ve7MmTPJ+uTkZK7t33DDDcn61q1bG9aGhoaS6za77Nv4+HiyPjMz07D2+OOPJ9ft6+vLVV8MOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM81/ltm3blqyfPXs2WV+1alWyfvz48WT95ptvblh7+umnk+sePXo0Wd+/f3+yPjY2lqxHx5EfCIrwA0ERfiAowg8ERfiBoAg/EBThB4JqOs5vZoOSvixpyt1vy5Ytk3REUq+kCUn3ufu7nWsTKanx8Lxj3Tt37kzWU+P4eW3ZsiVZ37hxY7J+9913N6yNjo4m1212/kKU3/P/SNI9ly3bI2nY3W+RNJw9BrCINA2/u49IeueyxZslHcruH5KU/hMNoOu0+5n/enc/L0nZ7YriWgJQho5/4Wdmu8ysbmb16enpTu8OQIvaDf8FM1spSdntVKMnuvuAu9fcvdbT09Pm7gAUrd3wH5N06WvgnZJeLKYdAGVpGn4ze07Sf0v6ezObNLOvSXpS0kYz+72kjdljAItI03F+d9/RoNR4EBWleuyxx9pet7e3N1l/4IEH2t52py1dujRZ37On8Qh0aj4BSRoZGWmrp8WEM/yAoAg/EBThB4Ii/EBQhB8IivADQXHp7kXgyJEjyfqpU6fa3vbhw4eT9dWrV7e9bXQ3jvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/F1gfHw8WX/wwQfb3nazS2s3m4J7MWt2+e3oOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM83eBkydPJuvvvtv+7OdDQ0PJ+mIe55+aajhRlCTp4MGDDWvXXJP+r9/f399OS4sKR34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrpOL+ZDUr6sqQpd78tW9Yv6V8kTWdPe9TdX+pUk1e7PFNsS9KWLVsa1tauXZtr21V67733kvVNmza1ve0lS5Yk6319fW1ve7Fo5cj/I0n3LLD8++6+JvtH8IFFpmn43X1E0jsl9AKgRHk+8z9kZr82s0Ezu66wjgCUot3w/0DS5yWtkXRe0ncbPdHMdplZ3czq09PTjZ4GoGRthd/dL7j7h+7+kaQfSlqXeO6Au9fcvdbT09NunwAK1lb4zWzlvIdbJbU/TSyASrQy1PecpDslLTezSUn7JN1pZmskuaQJSV/vYI8AOqBp+N19xwKLn+1AL1et7du3J+tnz57Ntf0NGzbkWr9bPfzww8n62NhY29vev39/2+teLTjDDwiK8ANBEX4gKMIPBEX4gaAIPxAUl+4uwSuvvJJr/W3btiXru3fvzrX9qgwMDCTrhw4dyrX91E+dH3nkkVzbvhpw5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnb9GJEyca1pr9pHZmZibXvp944olc61dp69atDWtHjx7t6L6v1p86F4UjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTh/ptlYet5ptFNuv/32ZH3ZsmUd23czVb4u1157bbI+PDycrC/m6cnLwJEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4JqOs5vZqsk/VjSDZI+kjTg7gfMbJmkI5J6JU1Ius/d3+1cq511/PjxyvY9Pj6erPf39yfrqfMAnnnmmeS6b731VrJepQMHDiTrjOPn08qRf1bSt9z9HyT9o6RvmNmtkvZIGnb3WyQNZ48BLBJNw+/u5919LLv/vqTTkm6UtFnSpSlVDklqPD0KgK5zRZ/5zaxX0hck/UrS9e5+Xpr7AyFpRdHNAeiclsNvZp+R9DNJ33T3P17BervMrG5m9enp6XZ6BNABLYXfzD6lueAfdvefZ4svmNnKrL5S0tRC67r7gLvX3L3W09NTRM8ACtA0/GZmkp6VdNrdvzevdEzSzuz+TkkvFt8egE5p5Se96yV9VdLrZnYyW/aopCcl/dTMvibpD5K+0pkWcfDgwapb6IihoaFkvdnU5Minafjd/ZeSrEH57mLbAVAWzvADgiL8QFCEHwiK8ANBEX4gKMIPBMWluzPNfjb76quvNqzNzs4W3M3icccddyTro6OjJXWCK8WRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpw/09fXl6zv27evYa3ZNNYzMzNt9VSGu+66K1nfu3dvsr5hw4Yi20GJOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFDm7qXtrFareb1eL21/QDS1Wk31er3RpfY/hiM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTVNPxmtsrMXjGz02b2GzN7OFveb2b/b2Yns3//1Pl2ARSllYt5zEr6lruPmdlnJY2a2ctZ7fvu/lTn2gPQKU3D7+7nJZ3P7r9vZqcl3djpxgB01hV95jezXklfkPSrbNFDZvZrMxs0s+sarLPLzOpmVp+ens7VLIDitBx+M/uMpJ9J+qa7/1HSDyR9XtIazb0z+O5C67n7gLvX3L3W09NTQMsAitBS+M3sU5oL/mF3/7kkufsFd//Q3T+S9ENJ6zrXJoCitfJtv0l6VtJpd//evOUr5z1tq6RTxbcHoFNa+bZ/vaSvSnrdzE5myx6VtMPM1khySROSvt6RDgF0RCvf9v9S0kK/D36p+HYAlIUz/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GVOkW3mU1L+r95i5ZLulhaA1emW3vr1r4kemtXkb39rbu3dL28UsP/iZ2b1d29VlkDCd3aW7f2JdFbu6rqjbf9QFCEHwiq6vAPVLz/lG7trVv7kuitXZX0VulnfgDVqfrID6AilYTfzO4xs/8xszNmtqeKHhoxswkzez2bebhecS+DZjZlZqfmLVtmZi+b2e+z2wWnSauot66YuTkxs3Slr123zXhd+tt+M1si6X8lbZQ0Kek1STvc/belNtKAmU1Iqrl75WPCZtYn6U+Sfuzut2XLviPpHXd/MvvDeZ27/2uX9NYv6U9Vz9ycTSizcv7M0pK2SPpnVfjaJfq6TxW8blUc+ddJOuPub7j7nyUNSdpcQR9dz91HJL1z2eLNkg5l9w9p7j9P6Rr01hXc/by7j2X335d0aWbpSl+7RF+VqCL8N0o6O+/xpLprym+X9AszGzWzXVU3s4Drs2nTL02fvqLifi7XdObmMl02s3TXvHbtzHhdtCrCv9DsP9005LDe3e+QtEnSN7K3t2hNSzM3l2WBmaW7QrszXhetivBPSlo17/HnJJ2roI8Fufu57HZK0gvqvtmHL1yaJDW7naq4n7/qppmbF5pZWl3w2nXTjNdVhP81SbeY2Woz+7Sk7ZKOVdDHJ5jZ0uyLGJnZUklfUvfNPnxM0s7s/k5JL1bYy8d0y8zNjWaWVsWvXbfNeF3JST7ZUMa/SVoiadDdv116Ewsws7/T3NFempvE9CdV9mZmz0m6U3O/+rogaZ+ko5J+KukmSX+Q9BV3L/2Ltwa93am5t65/nbn50mfsknv7oqRXJb0u6aNs8aOa+3xd2WuX6GuHKnjdOMMPCIoz/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPUX7hTvA5U3NsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking whether Classified Correctly or not for any random index\n",
    "\n",
    "rand_ind = np.random.randint(X_Alph_test.shape[0])\n",
    "plt.imshow(X_Alph_test[rand_ind].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "print(\"True Label: \",chr(np.argmax(Y_Alph_test[rand_ind])+65))\n",
    "\n",
    "print(\"Predicted Label: \",chr(predictions_Alph[rand_ind]+65))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason for this kind of accuracy on test data:\n",
    "1. OverFitting : Since we are getting 100 percent accuracy for train data that means it classified all the input correctly as it was trained , but when done with some variation of data i.e. test data we are getting less accuracy.\n",
    "2. Data Problem: Since the data for train and test data is done with random shuffling ,it may be possible that there are very less number of classes went to train data and remaining are there in test data. So , examples in test data may be new to the model , hence it classifies it wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for hidden layer of size 32 (TEST_DATA): : 57.49039692701664\n",
      "Accuracy for hidden layer of size 32 (TRAIN_DATA):: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for hidden layer of size 32 (TEST_DATA): :\"\n",
    "      ,Accuracy(predictions_Alph,Y_Alph_test))\n",
    "predictions1_Alph = predict(parameters_Alph, X_Alph_train)\n",
    "print(\"Accuracy for hidden layer of size 32 (TRAIN_DATA)::\"\n",
    "      ,Accuracy(predictions1_Alph,Y_Alph_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix of Shape:  (4, 2)\n",
      "Label Matrix of Shape:  (4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Preparing input and target for XOR\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,1,1,0]\n",
    "# Converting into numpy array\n",
    "X_XOR = np.array(X)\n",
    "Y_XOR = np.array(Y)\n",
    "\n",
    "# Converting into one hpt vector as the model uses softmax function\n",
    "Y_XOR = one_hot(Y_XOR,2)\n",
    "\n",
    "print(\"Input Matrix of Shape: \",X_XOR.shape)\n",
    "print(\"Label Matrix of Shape: \",Y_XOR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** X **********\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "**** Y one-hot encoded ****\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"********** X **********\")\n",
    "print(X_XOR)\n",
    "print(\"**** Y one-hot encoded ****\")\n",
    "print(Y_XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN Model with Xor Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Perceptron can not backpropagate because signum function is not differentiable.Hence , using the xor dataset in the model with number of hidden units as 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.856916\n",
      "Cost after iteration 1000: 0.691553\n",
      "Cost after iteration 2000: 0.640460\n",
      "Cost after iteration 3000: 0.182156\n",
      "Cost after iteration 4000: 0.050058\n",
      "Cost after iteration 5000: 0.026661\n"
     ]
    }
   ],
   "source": [
    "# Train a model with a n_h number of unit in hidden layer\n",
    "parameters_XOR=nn_model(X_XOR,Y_XOR,n_h=2,\n",
    "                        num_iterations=5000,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Getting the Predictions\n",
    "predictions_XOR = predict(parameters_XOR, X_XOR)\n",
    "print(\"Predictions: \",predictions_XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for hidden layer of size 2: : 100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for hidden layer of size 2: :\",\n",
    "      Accuracy(predictions_XOR,Y_XOR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
